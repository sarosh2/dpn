{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cead7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from utils import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b3c212",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1090548/1971492463.py:6: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Age'].fillna(df['Age'].median(), inplace=True)\n",
      "/tmp/ipykernel_1090548/1971492463.py:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# Load Titanic dataset (assuming it's saved locally as 'titanic.csv')\n",
    "df = pd.read_csv('Titanic-Dataset.csv')\n",
    "\n",
    "# Preprocess Data (fill missing values, encode categorical variables)\n",
    "df['Age'].fillna(df['Age'].median(), inplace=True)\n",
    "df['Embarked'].fillna(df['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Convert 'Sex' to binary (male = 0, female = 1)\n",
    "df['Sex'] = LabelEncoder().fit_transform(df['Sex'])\n",
    "\n",
    "# Convert 'Embarked' to numeric\n",
    "df['Embarked'] = LabelEncoder().fit_transform(df['Embarked'])\n",
    "\n",
    "# Drop columns not needed for prediction\n",
    "df = df.drop(['Name', 'Ticket', 'Cabin'], axis=1)\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop('Survived', axis=1).values\n",
    "y = df['Survived'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd1e57c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd6e18c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create DataLoader for batching\n",
    "train_data = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_data = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2a693b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)\n",
    "\n",
    "# Initialize the model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 1  # You can experiment with this value\n",
    "output_size = 2    # Binary classification (survived or not)\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "75a4d91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss is typically used for classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent with momentum)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a7220d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Total_Time: 0.2191 Average_Time_per_batch: 0.0095 Train_Accuracy: 0.6264 Train_Loss: 0.6746 \n",
      "Epoch: 2 Total_Time: 0.0312 Average_Time_per_batch: 0.0014 Train_Accuracy: 0.6250 Train_Loss: 0.6662 \n",
      "Epoch: 3 Total_Time: 0.0320 Average_Time_per_batch: 0.0014 Train_Accuracy: 0.6250 Train_Loss: 0.6624 \n",
      "Epoch: 4 Total_Time: 0.0327 Average_Time_per_batch: 0.0014 Train_Accuracy: 0.6236 Train_Loss: 0.6611 \n",
      "Epoch: 5 Total_Time: 0.0341 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.6236 Train_Loss: 0.6605 \n",
      "Epoch: 6 Total_Time: 0.0364 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.6236 Train_Loss: 0.6599 \n",
      "Epoch: 7 Total_Time: 0.0366 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.6236 Train_Loss: 0.6592 \n",
      "Epoch: 8 Total_Time: 0.0383 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6236 Train_Loss: 0.6585 \n",
      "Epoch: 9 Total_Time: 0.0377 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.6236 Train_Loss: 0.6574 \n",
      "Epoch: 10 Total_Time: 0.0393 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6236 Train_Loss: 0.6562 \n",
      "Epoch: 11 Total_Time: 0.0385 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6236 Train_Loss: 0.6542 \n",
      "Epoch: 12 Total_Time: 0.0384 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6236 Train_Loss: 0.6515 \n",
      "Epoch: 13 Total_Time: 0.0385 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6236 Train_Loss: 0.6481 \n",
      "Epoch: 14 Total_Time: 0.0396 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6236 Train_Loss: 0.6444 \n",
      "Epoch: 15 Total_Time: 0.0396 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6362 Train_Loss: 0.6401 \n",
      "Epoch: 16 Total_Time: 0.0396 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6699 Train_Loss: 0.6361 \n",
      "Epoch: 17 Total_Time: 0.0397 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6784 Train_Loss: 0.6318 \n",
      "Epoch: 18 Total_Time: 0.0402 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6882 Train_Loss: 0.6262 \n",
      "Epoch: 19 Total_Time: 0.0423 Average_Time_per_batch: 0.0018 Train_Accuracy: 0.7022 Train_Loss: 0.6196 \n",
      "Epoch: 20 Total_Time: 0.0396 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.7093 Train_Loss: 0.6123 \n",
      "Test_Accuracy:  0.6983 Test_Loss:  0.624313619549714\n",
      "Peak GPU memory: 17.06 MB\n"
     ]
    }
   ],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model, train_loader, None, test_loader, 20, optimizer, criterion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32b29ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "hidden_dims = [hidden_size, hidden_size, output_size]\n",
    "total = sum(hidden_dims)\n",
    "\n",
    "blocks = len(hidden_dims)\n",
    "features = input_size\n",
    "neural_blocks = []\n",
    "for dim in hidden_dims:\n",
    "    std_dev = torch.sqrt(torch.tensor(1 / features)).to(device)\n",
    "    neural_blocks.append(torch.randn(dim, features).to(device) * std_dev)\n",
    "    features += dim\n",
    "\n",
    "feature_blocks = []\n",
    "features_start = 0\n",
    "for i in range(len(neural_blocks)):\n",
    "    features_end = neural_blocks[i].shape[1]\n",
    "    block = neural_blocks[i][:, features_start:]\n",
    "    for j in range(i + 1, len(neural_blocks)):\n",
    "        block = torch.cat((block, neural_blocks[j][:, features_start:features_end]), dim=0)\n",
    "    feature_blocks.append(nn.Parameter(block))\n",
    "    features_start = features_end\n",
    "\n",
    "biases = biases = nn.Parameter(torch.empty(total).uniform_(0.0, 1.0)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fbd57331",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpn_2.dpn import DPN as DPN_2\n",
    "    \n",
    "class DPN_Softmax(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims, output_size):\n",
    "        super(DPN_Softmax, self).__init__()\n",
    "        self.fc1 =  DPN_2(input_size, sum(hidden_dims), output_size, True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.softmax(x)\n",
    "    \n",
    "model = DPN_Softmax(input_size, hidden_dims, output_size).to(device)\n",
    "model.fc1.weights.extend(feature_blocks)\n",
    "model.fc1.biases = biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d0b3c960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss is typically used for classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent with momentum)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "620f3b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Total_Time: 0.1629 Average_Time_per_batch: 0.0071 Train_Accuracy: 0.6826 Train_Loss: 0.6830 \n",
      "Epoch: 2 Total_Time: 0.0685 Average_Time_per_batch: 0.0030 Train_Accuracy: 0.6868 Train_Loss: 0.6819 \n",
      "Epoch: 3 Total_Time: 0.0713 Average_Time_per_batch: 0.0031 Train_Accuracy: 0.6966 Train_Loss: 0.6804 \n",
      "Epoch: 4 Total_Time: 0.0737 Average_Time_per_batch: 0.0032 Train_Accuracy: 0.6994 Train_Loss: 0.6791 \n",
      "Epoch: 5 Total_Time: 0.0747 Average_Time_per_batch: 0.0032 Train_Accuracy: 0.7008 Train_Loss: 0.6778 \n",
      "Epoch: 6 Total_Time: 0.0754 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7037 Train_Loss: 0.6765 \n",
      "Epoch: 7 Total_Time: 0.0747 Average_Time_per_batch: 0.0032 Train_Accuracy: 0.7051 Train_Loss: 0.6752 \n",
      "Epoch: 8 Total_Time: 0.0815 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.7051 Train_Loss: 0.6739 \n",
      "Epoch: 9 Total_Time: 0.0749 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7065 Train_Loss: 0.6726 \n",
      "Epoch: 10 Total_Time: 0.0749 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7093 Train_Loss: 0.6714 \n",
      "Epoch: 11 Total_Time: 0.0748 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7149 Train_Loss: 0.6701 \n",
      "Epoch: 12 Total_Time: 0.0755 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7177 Train_Loss: 0.6688 \n",
      "Epoch: 13 Total_Time: 0.0747 Average_Time_per_batch: 0.0032 Train_Accuracy: 0.7191 Train_Loss: 0.6676 \n",
      "Epoch: 14 Total_Time: 0.0755 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7205 Train_Loss: 0.6664 \n",
      "Epoch: 15 Total_Time: 0.0748 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7247 Train_Loss: 0.6650 \n",
      "Epoch: 16 Total_Time: 0.0753 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7247 Train_Loss: 0.6638 \n",
      "Epoch: 17 Total_Time: 0.0746 Average_Time_per_batch: 0.0032 Train_Accuracy: 0.7233 Train_Loss: 0.6626 \n",
      "Epoch: 18 Total_Time: 0.0753 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7261 Train_Loss: 0.6613 \n",
      "Epoch: 19 Total_Time: 0.0751 Average_Time_per_batch: 0.0033 Train_Accuracy: 0.7331 Train_Loss: 0.6600 \n",
      "Epoch: 20 Total_Time: 0.0747 Average_Time_per_batch: 0.0032 Train_Accuracy: 0.7388 Train_Loss: 0.6588 \n",
      "Test_Accuracy:  0.7207 Test_Loss:  0.6562142415419637\n",
      "Peak GPU memory: 17.05 MB\n"
     ]
    }
   ],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model, train_loader, None, test_loader, 20, optimizer, criterion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "640a11f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpn_2.dpn import DPN as DPN_2\n",
    "    \n",
    "class DPN_Softmax(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims, output_size):\n",
    "        super(DPN_Softmax, self).__init__()\n",
    "        self.fc1 =  DPN_2(input_size, sum(hidden_dims), output_size, False)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        return self.softmax(x)\n",
    "    \n",
    "model = DPN_Softmax(input_size, hidden_dims, output_size).to(device)\n",
    "model.fc1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5bc4b8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss is typically used for classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent with momentum)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0b46eaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Total_Time: 0.0816 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.5716 Train_Loss: 0.6948 \n",
      "Epoch: 2 Total_Time: 0.0816 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.5730 Train_Loss: 0.6921 \n",
      "Epoch: 3 Total_Time: 0.0814 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.5702 Train_Loss: 0.6888 \n",
      "Epoch: 4 Total_Time: 0.0834 Average_Time_per_batch: 0.0036 Train_Accuracy: 0.5730 Train_Loss: 0.6854 \n",
      "Epoch: 5 Total_Time: 0.0814 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.5815 Train_Loss: 0.6821 \n",
      "Epoch: 6 Total_Time: 0.0812 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.5885 Train_Loss: 0.6787 \n",
      "Epoch: 7 Total_Time: 0.0805 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.5955 Train_Loss: 0.6757 \n",
      "Epoch: 8 Total_Time: 0.0806 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6025 Train_Loss: 0.6728 \n",
      "Epoch: 9 Total_Time: 0.0805 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6053 Train_Loss: 0.6699 \n",
      "Epoch: 10 Total_Time: 0.0807 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6110 Train_Loss: 0.6672 \n",
      "Epoch: 11 Total_Time: 0.0811 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6124 Train_Loss: 0.6646 \n",
      "Epoch: 12 Total_Time: 0.0815 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6166 Train_Loss: 0.6621 \n",
      "Epoch: 13 Total_Time: 0.0812 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6194 Train_Loss: 0.6595 \n",
      "Epoch: 14 Total_Time: 0.0805 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6180 Train_Loss: 0.6570 \n",
      "Epoch: 15 Total_Time: 0.0805 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6236 Train_Loss: 0.6548 \n",
      "Epoch: 16 Total_Time: 0.0806 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6250 Train_Loss: 0.6523 \n",
      "Epoch: 17 Total_Time: 0.0839 Average_Time_per_batch: 0.0036 Train_Accuracy: 0.6306 Train_Loss: 0.6497 \n",
      "Epoch: 18 Total_Time: 0.0809 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6376 Train_Loss: 0.6472 \n",
      "Epoch: 19 Total_Time: 0.0804 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6390 Train_Loss: 0.6447 \n",
      "Epoch: 20 Total_Time: 0.0811 Average_Time_per_batch: 0.0035 Train_Accuracy: 0.6433 Train_Loss: 0.6422 \n",
      "Test_Accuracy:  0.6872 Test_Loss:  0.6249933622402852\n",
      "Peak GPU memory: 17.06 MB\n"
     ]
    }
   ],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model, train_loader, None, test_loader, 20, optimizer, criterion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d58e721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpn_3.dpn import DPN as DPN_2\n",
    "class DPN_Softmax(nn.Module):\n",
    "    def __init__(self, input_size, hidden_dims, output_size):\n",
    "        super(DPN_Softmax, self).__init__()\n",
    "        self.fc1 =  DPN_2(input_size, sum(hidden_dims), output_size, True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.fc1(x))\n",
    "        return self.softmax(x)\n",
    "    \n",
    "model = DPN_Softmax(input_size, hidden_dims, output_size).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "225bdb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CrossEntropyLoss is typically used for classification problems\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (Stochastic Gradient Descent with momentum)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "11011d27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Total_Time: 0.0427 Average_Time_per_batch: 0.0019 Train_Accuracy: 0.4986 Train_Loss: 0.6897 \n",
      "Epoch: 2 Total_Time: 0.0395 Average_Time_per_batch: 0.0017 Train_Accuracy: 0.6587 Train_Loss: 0.6614 \n",
      "Epoch: 3 Total_Time: 0.0377 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.6924 Train_Loss: 0.6360 \n",
      "Epoch: 4 Total_Time: 0.0362 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.7205 Train_Loss: 0.6127 \n",
      "Epoch: 5 Total_Time: 0.0361 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.7570 Train_Loss: 0.5942 \n",
      "Epoch: 6 Total_Time: 0.0366 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.7851 Train_Loss: 0.5780 \n",
      "Epoch: 7 Total_Time: 0.0350 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8020 Train_Loss: 0.5651 \n",
      "Epoch: 8 Total_Time: 0.0349 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8062 Train_Loss: 0.5547 \n",
      "Epoch: 9 Total_Time: 0.0365 Average_Time_per_batch: 0.0016 Train_Accuracy: 0.8048 Train_Loss: 0.5463 \n",
      "Epoch: 10 Total_Time: 0.0353 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8118 Train_Loss: 0.5401 \n",
      "Epoch: 11 Total_Time: 0.0349 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8062 Train_Loss: 0.5357 \n",
      "Epoch: 12 Total_Time: 0.0348 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8090 Train_Loss: 0.5315 \n",
      "Epoch: 13 Total_Time: 0.0354 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8104 Train_Loss: 0.5286 \n",
      "Epoch: 14 Total_Time: 0.0352 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8076 Train_Loss: 0.5260 \n",
      "Epoch: 15 Total_Time: 0.0349 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8076 Train_Loss: 0.5238 \n",
      "Epoch: 16 Total_Time: 0.0348 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8104 Train_Loss: 0.5220 \n",
      "Epoch: 17 Total_Time: 0.0355 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8104 Train_Loss: 0.5202 \n",
      "Epoch: 18 Total_Time: 0.0349 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8104 Train_Loss: 0.5186 \n",
      "Epoch: 19 Total_Time: 0.0348 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8118 Train_Loss: 0.5173 \n",
      "Epoch: 20 Total_Time: 0.0348 Average_Time_per_batch: 0.0015 Train_Accuracy: 0.8104 Train_Loss: 0.5159 \n",
      "Test_Accuracy:  0.7877 Test_Loss:  0.515913952995279\n",
      "Peak GPU memory: 18.11 MB\n"
     ]
    }
   ],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model, train_loader, None, test_loader, 20, optimizer, criterion, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf1ab28",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saroshgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
