{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4769d4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2870c2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the label mapping file and validation annotations\n",
    "LABEL_MAPPING_FILE = \"data/imagenet_dataset/LOC_synset_mapping.txt\"\n",
    "VAL_ANNOTATIONS_FILE = \"data/imagenet_dataset/LOC_val_solution.csv\"\n",
    "\n",
    "# Load the label mapping from LOC_synset_mapping.txt\n",
    "def load_label_mapping():\n",
    "    wnid_to_idx = {}\n",
    "    idx_to_wnid = {}\n",
    "    with open(LABEL_MAPPING_FILE, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for idx, line in enumerate(lines):\n",
    "            wnid = line.strip().split()[0]\n",
    "            wnid_to_idx[wnid] = idx\n",
    "            idx_to_wnid[idx] = wnid\n",
    "    return wnid_to_idx, idx_to_wnid\n",
    "\n",
    "# Load the validation annotations from LOC_val_solution.csv\n",
    "def load_val_annotations():\n",
    "    val_annotations = pd.read_csv(VAL_ANNOTATIONS_FILE)\n",
    "    val_annotations['PredictionString'] = val_annotations['PredictionString'].apply(lambda x: x.split()[0])\n",
    "    return val_annotations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8449ed7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageNetDataset(Dataset):\n",
    "    def __init__(self, data_dir, label_mapping, transform=None, is_train=True):\n",
    "        self.data_dir = data_dir\n",
    "        self.label_mapping = label_mapping\n",
    "        self.transform = transform\n",
    "        self.is_train = is_train\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load the train or validation data based on `is_train`\n",
    "        if is_train:\n",
    "            # Get all image paths and labels from train folder\n",
    "            for wnid in os.listdir(data_dir):\n",
    "                wnid_folder = os.path.join(data_dir, wnid)\n",
    "                if os.path.isdir(wnid_folder):\n",
    "                    count = 0\n",
    "                    for img_file in os.listdir(wnid_folder):\n",
    "                        if img_file.endswith('.JPEG'):\n",
    "                            self.image_paths.append(os.path.join(wnid_folder, img_file))\n",
    "                            self.labels.append(self.label_mapping[wnid])\n",
    "                            count += 1\n",
    "                            if count >= 1000:  # Limit to 1000 images per class\n",
    "                                break\n",
    "        else:\n",
    "            # Get the validation images and their corresponding labels from LOC_val_solution.csv\n",
    "            val_annotations = load_val_annotations()\n",
    "            for _, row in val_annotations.iterrows():\n",
    "                img_file = row['ImageId'] + '.JPEG'\n",
    "                wnid = row['PredictionString']\n",
    "                self.image_paths.append(os.path.join(data_dir, img_file))\n",
    "                self.labels.append(self.label_mapping[wnid])\n",
    "        \n",
    "        self.labels = torch.tensor(self.labels)\n",
    "\n",
    "        num_images = len(self.image_paths)\n",
    "        self.images = torch.empty((num_images, 3, 224, 224), dtype=torch.float32)\n",
    "\n",
    "        for idx, path in enumerate(self.image_paths):\n",
    "            image = Image.open(path).convert(\"RGB\")\n",
    "            if self.transform:\n",
    "                image = self.transform(image)\n",
    "            self.images[idx] = image\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels[idx]\n",
    "        image = self.images[idx]\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d480b33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and std for ImageNet\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_transforms = T.Compose([\n",
    "    T.RandomResizedCrop(224, interpolation=InterpolationMode.BICUBIC),\n",
    "    T.RandomHorizontalFlip(p=0.5),\n",
    "    T.RandAugment(num_ops=2, magnitude=9),  # can tune magnitude\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n",
    "\n",
    "val_transforms = T.Compose([\n",
    "    T.Resize(256, interpolation=InterpolationMode.BICUBIC),\n",
    "    T.CenterCrop(224),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "275e876f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the label mapping\n",
    "wnid_to_idx, idx_to_wnid = load_label_mapping()\n",
    "\n",
    "# Define the paths to the dataset\n",
    "train_data_dir = \"data/imagenet_dataset/ILSVRC/Data/CLS-LOC/train\"\n",
    "val_data_dir = \"data/imagenet_dataset/ILSVRC/Data/CLS-LOC/val\"\n",
    "\n",
    "# Create the train and test datasets\n",
    "train_dataset = ImageNetDataset(data_dir=train_data_dir, label_mapping=wnid_to_idx, transform=train_transforms, is_train=True)\n",
    "val_dataset = ImageNetDataset(data_dir=val_data_dir, label_mapping=wnid_to_idx, transform=val_transforms, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05b6c04a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train and test dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True, num_workers=16, pin_memory= True, persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False, num_workers=16, pin_memory=True, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6e26d44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for images, labels in train_loader:\\n    print(images.shape, labels.shape)\\n\\nfor images, labels in val_loader:\\n    print(images.shape, labels.shape)'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example usage\n",
    "'''for images, labels in train_loader:\n",
    "    print(images.shape, labels.shape)\n",
    "\n",
    "for images, labels in val_loader:\n",
    "    print(images.shape, labels.shape)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c05409e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 3                # change to 3 if you use CIFAR10 dataset\n",
    "image_size = 224                # change to 32 if you use CIFAR10 dataset\n",
    "num_classes = 1000\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "patch_size = 16         # Each patch is 16x16, so 14x14 = 196 patches per image\n",
    "hidden_dim = 768       # Token-mixing MLP hidden dim (formerly token_dim)\n",
    "tokens_mlp_dim = 384    # Tokens MLP dim\n",
    "channels_mlp_dim = 3072 # Channels MLP dim\n",
    "num_blocks = 12         # Number of Mixer layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c5237c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from utils import train\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91117fe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using 4 GPUs!\n"
     ]
    }
   ],
   "source": [
    "from MLP_Mixer import MLPMixer\n",
    "model = MLPMixer(in_channels=in_channels, embedding_dim=hidden_dim, num_classes=num_classes, patch_size=patch_size, image_size=image_size, depth=num_blocks, token_intermediate_dim=tokens_mlp_dim, channel_intermediate_dim=channels_mlp_dim)\n",
    "# If you have more than one GPU, wrap the model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)  # Wrap the model for multi-GPU usage\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f93a9169",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Loss Function (with label smoothing)\n",
    "class LabelSmoothingCrossEntropy(nn.Module):\n",
    "    def __init__(self, smoothing=0.1):\n",
    "        super(LabelSmoothingCrossEntropy, self).__init__()\n",
    "        self.smoothing = smoothing\n",
    "\n",
    "    def forward(self, x, target):\n",
    "        log_probs = torch.nn.functional.log_softmax(x, dim=-1)\n",
    "        nll_loss = -log_probs.gather(dim=-1, index=target.unsqueeze(1)).squeeze(1)\n",
    "        smooth_loss = -log_probs.mean(dim=-1)\n",
    "        return ((1 - self.smoothing) * nll_loss + self.smoothing * smooth_loss).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f58d6fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = LabelSmoothingCrossEntropy(smoothing=0.1)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=3e-3, weight_decay=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91dac638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1 Total_Time: 10.9228 Average_Time_per_batch: 0.2731 Train_Accuracy: 0.0005 Train_Loss: 7.0185 Validation_Accuracy: 0.0011 Validation_Loss: 6.9354\n",
      "Epoch: 2 Total_Time: 3.4567 Average_Time_per_batch: 0.0864 Train_Accuracy: 0.0015 Train_Loss: 6.9002 Validation_Accuracy: 0.0020 Validation_Loss: 6.9077"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_metrics_3, val_metrics_3, test_metrics_3 \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/ediss_data/ediss4/sarosh/personal/pathnn/spn/utils.py:48\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, test_loader, epochs, optimizer, criterion, flatten, device)\u001b[0m\n\u001b[1;32m     45\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     46\u001b[0m batch_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m batch_start\n\u001b[0;32m---> 48\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m y_batch\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     49\u001b[0m preds \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     50\u001b[0m y_batch_labels \u001b[38;5;241m=\u001b[39m y_batch\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m y_batch\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m y_batch\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model, train_loader, val_loader, val_loader, 20, optimizer, criterion, False, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ec6a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785976b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from DPN_Mixer import MLPMixer as DPNMixer\n",
    "model = DPNMixer(in_channels=in_channels, embedding_dim=hidden_dim, num_classes=num_classes, patch_size=patch_size, image_size=image_size, depth=num_blocks, token_intermediate_dim=tokens_mlp_dim, channel_intermediate_dim=channels_mlp_dim)\n",
    "# If you have more than one GPU, wrap the model with DataParallel\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(f\"Using {torch.cuda.device_count()} GPUs!\")\n",
    "    model = nn.DataParallel(model)  # Wrap the model for multi-GPU usage\n",
    "\n",
    "# Move the model to the GPU\n",
    "model = model.to(device)\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model, train_loader, val_loader, val_loader, 20, optimizer, criterion, False, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb3c4f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saroshgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
