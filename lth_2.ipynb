{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c160eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79c300f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for x, y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model(x)\n",
    "        loss = criterion(preds, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "    return total_loss / len(loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b748fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to evaluate model\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            preds = model(x)\n",
    "            loss = criterion(preds, y)\n",
    "            total_loss += loss.item() * x.size(0)\n",
    "            correct += (preds.argmax(dim=1) == y).sum().item()\n",
    "    return total_loss / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8aa70fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to prune weights by percentage\n",
    "def prune_weights(model, mask, prune_percentage):\n",
    "    new_mask = []\n",
    "    with torch.no_grad():\n",
    "        for param, m in zip(model.parameters(), mask):\n",
    "            tensor = param.data.cpu().abs() * m\n",
    "            # Only prune if there are non-zero values in the tensor\n",
    "            non_zero_tensor = tensor[tensor > 0]\n",
    "            if non_zero_tensor.numel() > 0:\n",
    "                threshold = torch.quantile(non_zero_tensor, prune_percentage)\n",
    "                new_m = torch.where(tensor <= threshold, torch.zeros_like(m), m)\n",
    "            else:\n",
    "                new_m = m  # If no non-zero weights, keep the original mask\n",
    "            param.data.mul_(new_m.to(param.device))\n",
    "            new_mask.append(new_m)\n",
    "    return new_mask\n",
    "\n",
    "\n",
    "# Create mask for initial weights\n",
    "def init_mask(model):\n",
    "    return [torch.ones_like(param, device='cpu') for param in model.parameters()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91d266e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lottery Ticket Hypothesis implementation\n",
    "def lottery_ticket(model, train_loader, val_loader, optimizer, criterion,\n",
    "                   device='cuda', epochs=2, prune_percentage=0.2, rounds=10, final_epochs=50):\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Save original weights\n",
    "    original_weights = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0\n",
    "    best_mask = None\n",
    "\n",
    "    mask = init_mask(model)\n",
    "\n",
    "    for round in range(rounds):\n",
    "        print(f'\\n--- Round {round+1} ---')\n",
    "\n",
    "        # Reset weights to original initialization with mask applied\n",
    "        model.load_state_dict(original_weights)\n",
    "        with torch.no_grad():\n",
    "            for param, m in zip(model.parameters(), mask):\n",
    "                param.data.mul_(m.to(device))\n",
    "\n",
    "        # Retrain\n",
    "        for epoch in range(epochs):\n",
    "            train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "            val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "            print(f'Epoch {epoch+1}/{epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            best_mask = copy.deepcopy(mask)\n",
    "\n",
    "        # Prune weights\n",
    "        mask = prune_weights(model, mask, prune_percentage)\n",
    "\n",
    "    print(f'\\nBest validation accuracy: {best_acc:.4f}')\n",
    "\n",
    "    # Final training with best mask\n",
    "    print('\\n--- Final training with best mask ---')\n",
    "    model.load_state_dict(original_weights)\n",
    "    with torch.no_grad():\n",
    "        for param, m in zip(model.parameters(), best_mask):\n",
    "            param.data.mul_(m.to(device))\n",
    "\n",
    "    for epoch in range(final_epochs):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
    "        print(f'Final Epoch {epoch+1}/{final_epochs} - Train loss: {train_loss:.4f}, Val loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}')\n",
    "\n",
    "# Example Usage (assuming all components provided):\n",
    "# lottery_ticket(model, train_loader, val_loader, optimizer, criterion, device='cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "151b0d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e045d991",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)  # Set the seed for NumPy\n",
    "\n",
    "# Set the seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# If you're using CUDA (GPU), set the seed for CUDA as well\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # For all GPUs (if you have more than one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20df6d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Download the dataset (OpenML: id 180)\n",
    "dataset = openml.datasets.get_dataset(180)\n",
    "X, y, _, _ = dataset.get_data(target=dataset.default_target_attribute, dataset_format='dataframe')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "81448657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Preprocessing: Numeric features only, standardize, encode labels (classes are 1-7)\n",
    "X_numeric = X.select_dtypes(include=[np.number])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_numeric.values.astype(np.float32))\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)  # Converts to 0...6\n",
    "y_encoded = y_encoded.astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e9110d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Train/test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_encoded, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7644576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. PyTorch Dataset\n",
    "class CovertypeDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X)\n",
    "        self.y = torch.from_numpy(y)\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds = CovertypeDataset(X_train, y_train)\n",
    "test_ds = CovertypeDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=256, shuffle=False)\n",
    "test_loader = DataLoader(test_ds, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6dee62ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dpn_3.dpn import DPN\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DPN(X_train.shape[1], 192 + len(le.classes_), len(le.classes_), False).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e397c365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Training setup\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71f5acc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Round 1 ---\n",
      "Epoch 1/2 - Train loss: 0.9540, Val loss: 0.8344, Val Acc: 0.6849\n",
      "Epoch 2/2 - Train loss: 0.8376, Val loss: 0.8101, Val Acc: 0.6932\n",
      "\n",
      "--- Round 2 ---\n",
      "Epoch 1/2 - Train loss: 0.9518, Val loss: 0.8337, Val Acc: 0.6848\n",
      "Epoch 2/2 - Train loss: 0.8384, Val loss: 0.8109, Val Acc: 0.6953\n",
      "\n",
      "--- Round 3 ---\n",
      "Epoch 1/2 - Train loss: 0.9371, Val loss: 0.8277, Val Acc: 0.6884\n",
      "Epoch 2/2 - Train loss: 0.8325, Val loss: 0.8049, Val Acc: 0.6991\n",
      "\n",
      "--- Round 4 ---\n",
      "Epoch 1/2 - Train loss: 0.9320, Val loss: 0.8278, Val Acc: 0.6869\n",
      "Epoch 2/2 - Train loss: 0.8318, Val loss: 0.8052, Val Acc: 0.6970\n",
      "\n",
      "--- Round 5 ---\n",
      "Epoch 1/2 - Train loss: 0.9322, Val loss: 0.8257, Val Acc: 0.6887\n",
      "Epoch 2/2 - Train loss: 0.8304, Val loss: 0.8042, Val Acc: 0.6971\n",
      "\n",
      "--- Round 6 ---\n",
      "Epoch 1/2 - Train loss: 0.9385, Val loss: 0.8279, Val Acc: 0.6876\n",
      "Epoch 2/2 - Train loss: 0.8323, Val loss: 0.8070, Val Acc: 0.6959\n",
      "\n",
      "--- Round 7 ---\n",
      "Epoch 1/2 - Train loss: 0.9493, Val loss: 0.8288, Val Acc: 0.6870\n",
      "Epoch 2/2 - Train loss: 0.8336, Val loss: 0.8065, Val Acc: 0.6967\n",
      "\n",
      "--- Round 8 ---\n",
      "Epoch 1/2 - Train loss: 0.9672, Val loss: 0.8346, Val Acc: 0.6863\n",
      "Epoch 2/2 - Train loss: 0.8379, Val loss: 0.8099, Val Acc: 0.6957\n",
      "\n",
      "--- Round 9 ---\n",
      "Epoch 1/2 - Train loss: 0.9841, Val loss: 0.8390, Val Acc: 0.6841\n",
      "Epoch 2/2 - Train loss: 0.8409, Val loss: 0.8135, Val Acc: 0.6937\n",
      "\n",
      "--- Round 10 ---\n",
      "Epoch 1/2 - Train loss: 1.0014, Val loss: 0.8446, Val Acc: 0.6816\n",
      "Epoch 2/2 - Train loss: 0.8460, Val loss: 0.8177, Val Acc: 0.6906\n",
      "\n",
      "Best validation accuracy: 0.6991\n",
      "\n",
      "--- Final training with best mask ---\n",
      "Final Epoch 1/50 - Train loss: 0.9244, Val loss: 0.8280, Val Acc: 0.6859\n",
      "Final Epoch 2/50 - Train loss: 0.8325, Val loss: 0.8051, Val Acc: 0.6987\n",
      "Final Epoch 3/50 - Train loss: 0.8126, Val loss: 0.7884, Val Acc: 0.7050\n",
      "Final Epoch 4/50 - Train loss: 0.7945, Val loss: 0.7741, Val Acc: 0.7103\n",
      "Final Epoch 5/50 - Train loss: 0.7771, Val loss: 0.7600, Val Acc: 0.7196\n",
      "Final Epoch 6/50 - Train loss: 0.7575, Val loss: 0.7421, Val Acc: 0.7296\n",
      "Final Epoch 7/50 - Train loss: 0.7380, Val loss: 0.7258, Val Acc: 0.7396\n",
      "Final Epoch 8/50 - Train loss: 0.7203, Val loss: 0.7102, Val Acc: 0.7487\n",
      "Final Epoch 9/50 - Train loss: 0.7037, Val loss: 0.6960, Val Acc: 0.7571\n",
      "Final Epoch 10/50 - Train loss: 0.6881, Val loss: 0.6836, Val Acc: 0.7631\n",
      "Final Epoch 11/50 - Train loss: 0.6742, Val loss: 0.6733, Val Acc: 0.7696\n",
      "Final Epoch 12/50 - Train loss: 0.6622, Val loss: 0.6648, Val Acc: 0.7744\n",
      "Final Epoch 13/50 - Train loss: 0.6511, Val loss: 0.6570, Val Acc: 0.7787\n",
      "Final Epoch 14/50 - Train loss: 0.6406, Val loss: 0.6496, Val Acc: 0.7815\n",
      "Final Epoch 15/50 - Train loss: 0.6306, Val loss: 0.6427, Val Acc: 0.7852\n",
      "Final Epoch 16/50 - Train loss: 0.6212, Val loss: 0.6363, Val Acc: 0.7875\n",
      "Final Epoch 17/50 - Train loss: 0.6123, Val loss: 0.6296, Val Acc: 0.7911\n",
      "Final Epoch 18/50 - Train loss: 0.6040, Val loss: 0.6232, Val Acc: 0.7959\n",
      "Final Epoch 19/50 - Train loss: 0.5964, Val loss: 0.6175, Val Acc: 0.7982\n",
      "Final Epoch 20/50 - Train loss: 0.5893, Val loss: 0.6125, Val Acc: 0.8012\n",
      "Final Epoch 21/50 - Train loss: 0.5826, Val loss: 0.6081, Val Acc: 0.8033\n",
      "Final Epoch 22/50 - Train loss: 0.5764, Val loss: 0.6043, Val Acc: 0.8057\n",
      "Final Epoch 23/50 - Train loss: 0.5705, Val loss: 0.6013, Val Acc: 0.8072\n",
      "Final Epoch 24/50 - Train loss: 0.5649, Val loss: 0.5989, Val Acc: 0.8097\n",
      "Final Epoch 25/50 - Train loss: 0.5595, Val loss: 0.5971, Val Acc: 0.8113\n",
      "Final Epoch 26/50 - Train loss: 0.5543, Val loss: 0.5958, Val Acc: 0.8124\n",
      "Final Epoch 27/50 - Train loss: 0.5492, Val loss: 0.5948, Val Acc: 0.8136\n",
      "Final Epoch 28/50 - Train loss: 0.5443, Val loss: 0.5939, Val Acc: 0.8149\n",
      "Final Epoch 29/50 - Train loss: 0.5395, Val loss: 0.5932, Val Acc: 0.8162\n",
      "Final Epoch 30/50 - Train loss: 0.5349, Val loss: 0.5927, Val Acc: 0.8172\n",
      "Final Epoch 31/50 - Train loss: 0.5306, Val loss: 0.5923, Val Acc: 0.8177\n",
      "Final Epoch 32/50 - Train loss: 0.5265, Val loss: 0.5919, Val Acc: 0.8184\n",
      "Final Epoch 33/50 - Train loss: 0.5225, Val loss: 0.5917, Val Acc: 0.8183\n",
      "Final Epoch 34/50 - Train loss: 0.5187, Val loss: 0.5915, Val Acc: 0.8192\n",
      "Final Epoch 35/50 - Train loss: 0.5152, Val loss: 0.5914, Val Acc: 0.8200\n",
      "Final Epoch 36/50 - Train loss: 0.5120, Val loss: 0.5914, Val Acc: 0.8198\n",
      "Final Epoch 37/50 - Train loss: 0.5091, Val loss: 0.5908, Val Acc: 0.8196\n",
      "Final Epoch 38/50 - Train loss: 0.5056, Val loss: 0.5909, Val Acc: 0.8196\n",
      "Final Epoch 39/50 - Train loss: 0.5029, Val loss: 0.5903, Val Acc: 0.8207\n",
      "Final Epoch 40/50 - Train loss: 0.4999, Val loss: 0.5906, Val Acc: 0.8217\n",
      "Final Epoch 41/50 - Train loss: 0.4968, Val loss: 0.5905, Val Acc: 0.8223\n",
      "Final Epoch 42/50 - Train loss: 0.4937, Val loss: 0.5902, Val Acc: 0.8224\n",
      "Final Epoch 43/50 - Train loss: 0.4910, Val loss: 0.5895, Val Acc: 0.8233\n",
      "Final Epoch 44/50 - Train loss: 0.4883, Val loss: 0.5904, Val Acc: 0.8235\n",
      "Final Epoch 45/50 - Train loss: 0.4855, Val loss: 0.5906, Val Acc: 0.8232\n",
      "Final Epoch 46/50 - Train loss: 0.4831, Val loss: 0.5907, Val Acc: 0.8241\n",
      "Final Epoch 47/50 - Train loss: 0.4804, Val loss: 0.5912, Val Acc: 0.8244\n",
      "Final Epoch 48/50 - Train loss: 0.4783, Val loss: 0.5904, Val Acc: 0.8252\n",
      "Final Epoch 49/50 - Train loss: 0.4759, Val loss: 0.5913, Val Acc: 0.8260\n",
      "Final Epoch 50/50 - Train loss: 0.4731, Val loss: 0.5893, Val Acc: 0.8274\n"
     ]
    }
   ],
   "source": [
    "lottery_ticket(model, train_loader, test_loader, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saroshgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
