{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get System Info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device Name: Tesla V100-PCIE-16GB\n",
      "Compute Capability: 7.0\n",
      "Total Memory: 15.77 GB\n"
     ]
    }
   ],
   "source": [
    "import cupy as cp\n",
    "import numpy as np\n",
    "\n",
    "np.set_printoptions(threshold=np.inf)  # Show all elements\n",
    "\n",
    "# Get the default device ID (usually 0)\n",
    "device_id = cp.cuda.runtime.getDevice()\n",
    "\n",
    "# Get device properties\n",
    "props = cp.cuda.runtime.getDeviceProperties(device_id)\n",
    "\n",
    "# Print device name\n",
    "device_name = props['name'].decode('utf-8')\n",
    "print(f\"Device Name: {device_name}\")\n",
    "\n",
    "# Print compute capability\n",
    "cc_major = props['major']\n",
    "cc_minor = props['minor']\n",
    "print(f\"Compute Capability: {cc_major}.{cc_minor}\")\n",
    "\n",
    "# Print total memory\n",
    "total_mem = props['totalGlobalMem'] / (1024 ** 3)  # Convert bytes to GB\n",
    "print(f\"Total Memory: {total_mem:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 18:59:09.088665: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-05-30 18:59:09.270750: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-05-30 18:59:10.565469: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-30 18:59:10.565564: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-05-30 18:59:10.565575: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train Shape:  (784, 48000)\n",
      "X_Val Shape:  (784, 12000)\n",
      "X_test Shape:  (784, 10000)\n",
      "Y_Train Shape:  (10, 48000)\n",
      "Y_Val Shape:  (10, 12000)\n",
      "Y_Test Shape:  (10, 10000)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import cupy as cp\n",
    "# Load the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "#flatten the dataset\n",
    "x_train_flat = x_train.reshape(x_train.shape[0], 28 * 28).T\n",
    "x_test_flat = x_test.reshape(x_test.shape[0], 28 * 28).T\n",
    "\n",
    "train_size = int(0.8 * x_train_flat.shape[1])  # 48000\n",
    "\n",
    "x_val = x_train_flat[:, train_size:]     # Last 12000 columns for validation\n",
    "x_train_flat = x_train_flat[:, :train_size]   # First 48000 columns for training\n",
    "\n",
    "print(\"X_train Shape: \", x_train_flat.shape)\n",
    "print(\"X_Val Shape: \", x_val.shape)\n",
    "print(\"X_test Shape: \", x_test_flat.shape)\n",
    "\n",
    "y_train = cp.eye(10)[y_train].T #convert to one hot encoded vectors\n",
    "\n",
    "y_val = y_train[:, train_size:]\n",
    "y_train = y_train[:, :train_size]\n",
    "\n",
    "print(\"Y_Train Shape: \", y_train.shape)\n",
    "print(\"Y_Val Shape: \", y_val.shape)\n",
    "\n",
    "y_test = cp.eye(10)[y_test].T #convert to one hot encoded vectors\n",
    "\n",
    "print(\"Y_Test Shape: \", y_test.shape)\n",
    "\n",
    "epochs = 10\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. spn_0. The object oriented approach with sequential forward and backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spn_0.spn import SPN as SPN_0\n",
    "import cupy as cp\n",
    "\n",
    "model_0 = SPN_0()\n",
    "cp.random.seed(42)\n",
    "\n",
    "input_features = 784 # for MNIST dataset\n",
    "hidden_nodes = 703\n",
    "output_nodes = 10\n",
    "\n",
    "#manually create the network structure\n",
    "for i in range(hidden_nodes):\n",
    "    model_0.create_node('Relu', input_features, 'input')\n",
    "\n",
    "for i in range(1, hidden_nodes):\n",
    "    for j in range(i + 1, hidden_nodes + 1):\n",
    "        model_0.add_connection(i, j)\n",
    "\n",
    "inputs = model_0.input_nodes.copy()\n",
    "for i in range(output_nodes):\n",
    "    new_node = model_0.create_node('None', input_features, 'input')\n",
    "    model_0.output_nodes.append(new_node)\n",
    "    model_0.vertices[new_node].output_size = 1\n",
    "\n",
    "    for node in inputs:\n",
    "        model_0.add_connection(node, new_node)\n",
    "        \n",
    "for i in range(hidden_nodes + 1, hidden_nodes + output_nodes):\n",
    "    for j in range(i + 1, hidden_nodes + output_nodes + 1):\n",
    "        model_0.add_connection(i, j)\n",
    "\n",
    "model_0.compile()\n",
    "#model_0.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract weights before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract weights from spn\n",
    "\n",
    "spn_weights = []\n",
    "biases = []\n",
    "for i, node in enumerate(model_0.input_nodes):\n",
    "    spn_weights.append(model_0.vertices[node].weights.copy())\n",
    "    biases.append(model_0.vertices[node].bias)\n",
    "\n",
    "max_pad = spn_weights[-1].shape[1]\n",
    "\n",
    "for i in range(len(spn_weights)):\n",
    "    row = cp.pad(spn_weights[i],  pad_width=((0, 0), (1, max_pad - spn_weights[i].shape[1])), mode='constant', constant_values=(biases[i], 0))\n",
    "    spn_weights[i] = row\n",
    "\n",
    "spn_weights = cp.vstack(spn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_metrics_0, val_metrics_0, test_metrics_0 = model_0.execute(epochs, batch_size, x_train_flat, y_train, x_val, y_val, x_test_flat, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. spn_1. The block based forward appproach with sequential backprop in cupy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use spn_0 weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "spn_1_weights = spn_weights.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spn_1.spn import SPN as SPN_1\n",
    "\n",
    "model_1 = SPN_1(input_features, hidden_nodes + output_nodes, output_nodes)\n",
    "model_1.set_weights(spn_1_weights)\n",
    "model_1.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#append 1s to multiply with biases\n",
    "ones_column_train = cp.ones((1, x_train_flat.shape[1]))\n",
    "x_train_flat_with_ones = cp.vstack((ones_column_train, x_train_flat))\n",
    "\n",
    "ones_column_val = cp.ones((1, x_val.shape[1]))\n",
    "x_val_with_ones = cp.vstack((ones_column_val, x_val))\n",
    "\n",
    "ones_column_test = cp.ones((1, x_test_flat.shape[1]))\n",
    "x_test_flat_with_ones = cp.vstack((ones_column_test, x_test_flat))\n",
    "\n",
    "alpha = 0.001\n",
    "beta_1 = 0.9\n",
    "beta_2 = 0.999\n",
    "epsilon = 1e-8\n",
    "t = 1  # Timestep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_metrics_1, val_metrics_1 = model_1.fit(x_train_flat_with_ones, y_train, x_val_with_ones, y_val, epochs, batch_size, alpha, beta_1, beta_2, epsilon)\n",
    "#test_metrics_1 = model_1.test(x_test_flat_with_ones, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. spn_2. Block based forward and backprop approach in pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def cp_to_torch_tensor(cp_array):\n",
    "    return torch.tensor(cp.asnumpy(cp_array)).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X_train = cp_to_torch_tensor(x_train_flat).T  # [N, 784]\n",
    "X_val = cp_to_torch_tensor(x_val).T\n",
    "X_test = cp_to_torch_tensor(x_test_flat).T\n",
    "\n",
    "Y_train = cp_to_torch_tensor(y_train).T  # [N]\n",
    "Y_val = cp_to_torch_tensor(y_val).T\n",
    "Y_test = cp_to_torch_tensor(y_test).T\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(TensorDataset(X_test, Y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spn_2.spn import SPN as SPN_2\n",
    "    \n",
    "model_2 = SPN_2(input_features, hidden_nodes + output_nodes, output_nodes, False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "spn_2_weights = []\n",
    "\n",
    "limits = spn_weights.shape[1] - cp.argmax(spn_weights[:, ::-1] != 0, axis=1)\n",
    "start = 1\n",
    "old_limit = 0\n",
    "\n",
    "for limit in limits:\n",
    "    if limit > old_limit:\n",
    "        old_limit = limit\n",
    "        spn_2_weights.append(nn.Parameter(torch.as_tensor(spn_weights[limit - 1 - input_features:, start:limit].copy().get()).float().cuda()))\n",
    "        start = limit\n",
    "\n",
    "spn_2_biases = nn.Parameter(torch.as_tensor(spn_weights[:, 0].get()).float().cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.weights.extend(spn_2_weights)\n",
    "model_2.biases = spn_2_biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ediss_data/ediss4/sarosh/personal/pathnn/spn/spn_2/spn.py:22: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if input_start_idx < nodes - output_size:\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "model_2 = torch.jit.trace(model_2, torch.randn(batch_size, input_features).cuda())\n",
    "optimizer = optim.Adam(model_2.parameters(), lr=alpha, betas=(beta_1, beta_2), eps=epsilon)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Total_Time: 119.7386 Average_Time_per_batch: 0.1597 Train_Accuracy: 0.9249 Train_Loss: 0.2447 Val_Accuracy: 0.9597 Val_Loss: 0.1292\n",
      "Epoch: 2 Total_Time: 117.7434 Average_Time_per_batch: 0.1570 Train_Accuracy: 0.9705 Train_Loss: 0.0958 Val_Accuracy: 0.9713 Val_Loss: 0.0970\n",
      "Epoch: 3 Total_Time: 118.1292 Average_Time_per_batch: 0.1575 Train_Accuracy: 0.9795 Train_Loss: 0.0645 Val_Accuracy: 0.9735 Val_Loss: 0.0890\n",
      "Epoch: 4 Total_Time: 118.0554 Average_Time_per_batch: 0.1574 Train_Accuracy: 0.9853 Train_Loss: 0.0458 Val_Accuracy: 0.9729 Val_Loss: 0.0943\n",
      "Epoch: 5 Total_Time: 117.7072 Average_Time_per_batch: 0.1569 Train_Accuracy: 0.9881 Train_Loss: 0.0370 Val_Accuracy: 0.9729 Val_Loss: 0.0954\n",
      "Epoch: 6 Total_Time: 117.8271 Average_Time_per_batch: 0.1571 Train_Accuracy: 0.9906 Train_Loss: 0.0300 Val_Accuracy: 0.9778 Val_Loss: 0.0892\n",
      "Epoch: 7 Total_Time: 117.9259 Average_Time_per_batch: 0.1572 Train_Accuracy: 0.9917 Train_Loss: 0.0235 Val_Accuracy: 0.9777 Val_Loss: 0.0867\n",
      "Epoch: 8 Total_Time: 118.1205 Average_Time_per_batch: 0.1575 Train_Accuracy: 0.9925 Train_Loss: 0.0232 Val_Accuracy: 0.9762 Val_Loss: 0.1015\n",
      "Epoch: 9 Total_Time: 117.8831 Average_Time_per_batch: 0.1572 Train_Accuracy: 0.9933 Train_Loss: 0.0197 Val_Accuracy: 0.9771 Val_Loss: 0.0975\n",
      "Epoch: 10 Total_Time: 117.9489 Average_Time_per_batch: 0.1573 Train_Accuracy: 0.9943 Train_Loss: 0.0168 Val_Accuracy: 0.9752 Val_Loss: 0.1140\n",
      "Test_Accuracy:  0.9777 Test_Loss:  0.09453997173129174\n",
      "Peak GPU memory: 257.85 MB\n"
     ]
    }
   ],
   "source": [
    "from utils import train\n",
    "\n",
    "train_metrics_2, val_metrics_2, test_metrics_2 = train(model_2, train_loader, val_loader, test_loader, epochs, optimizer, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spn_3.spn import SPN as SPN_3\n",
    "    \n",
    "model_3 = SPN_3(input_features, hidden_nodes + output_nodes, output_nodes, False).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_3 = torch.jit.trace(model_3, torch.randn(batch_size, input_features).cuda())\n",
    "optimizer = optim.Adam(model_3.parameters(), lr=alpha, betas=(beta_1, beta_2), eps=epsilon)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 Total_Time: 217.9252 Average_Time_per_batch: 0.2906 Train_Accuracy: 0.9269 Train_Loss: 0.2418 Val_Accuracy: 0.9618 Val_Loss: 0.1225\n",
      "Epoch: 2 Total_Time: 121.6688 Average_Time_per_batch: 0.1622 Train_Accuracy: 0.9698 Train_Loss: 0.0979 Val_Accuracy: 0.9694 Val_Loss: 0.1034\n",
      "Epoch: 3 Total_Time: 121.4814 Average_Time_per_batch: 0.1620 Train_Accuracy: 0.9789 Train_Loss: 0.0665 Val_Accuracy: 0.9731 Val_Loss: 0.0947\n",
      "Epoch: 4 Total_Time: 120.2777 Average_Time_per_batch: 0.1604 Train_Accuracy: 0.9854 Train_Loss: 0.0469 Val_Accuracy: 0.9724 Val_Loss: 0.0984\n",
      "Epoch: 5 Total_Time: 121.1118 Average_Time_per_batch: 0.1615 Train_Accuracy: 0.9879 Train_Loss: 0.0365 Val_Accuracy: 0.9706 Val_Loss: 0.1089\n",
      "Epoch: 6 Total_Time: 120.5235 Average_Time_per_batch: 0.1607 Train_Accuracy: 0.9910 Train_Loss: 0.0279 Val_Accuracy: 0.9728 Val_Loss: 0.1033\n",
      "Epoch: 7 Total_Time: 120.2340 Average_Time_per_batch: 0.1603 Train_Accuracy: 0.9915 Train_Loss: 0.0267 Val_Accuracy: 0.9752 Val_Loss: 0.0947\n",
      "Epoch: 8 Total_Time: 120.9942 Average_Time_per_batch: 0.1613 Train_Accuracy: 0.9930 Train_Loss: 0.0219 Val_Accuracy: 0.9761 Val_Loss: 0.0985\n",
      "Epoch: 9 Total_Time: 120.9546 Average_Time_per_batch: 0.1613 Train_Accuracy: 0.9933 Train_Loss: 0.0198 Val_Accuracy: 0.9758 Val_Loss: 0.0999\n",
      "Epoch: 10 Total_Time: 120.7798 Average_Time_per_batch: 0.1610 Train_Accuracy: 0.9948 Train_Loss: 0.0156 Val_Accuracy: 0.9762 Val_Loss: 0.1105\n",
      "Test_Accuracy:  0.9753 Test_Loss:  0.11415401333952323\n",
      "Peak GPU memory: 468.43 MB\n"
     ]
    }
   ],
   "source": [
    "train_metrics_3, val_metrics_3, test_metrics_3 = train(model_3, train_loader, val_loader, test_loader, epochs, optimizer, criterion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "saroshgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
